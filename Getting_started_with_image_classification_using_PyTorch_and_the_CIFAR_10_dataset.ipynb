{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGzSZk44qu2HVsBgv+TiI8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NikosKats/ColabFiles/blob/Getting_started_with_image_classification_using_PyTorch_and_the_CIFAR_10_dataset.ipynb/Getting_started_with_image_classification_using_PyTorch_and_the_CIFAR_10_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this blog post, we will be discussing how to load and preprocess the CIFAR-10 dataset using PyTorch, and how to train and evaluate a convolutional neural network (CNN) model for image classification.\n",
        "\n",
        "The CIFAR-10 dataset is a widely used dataset for image classification, which consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images. The 10 classes are airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck.\n",
        "\n",
        "First, we need to load the CIFAR-10 dataset and apply some data augmentation techniques, such as random horizontal flipping and random cropping. Data augmentation helps to increase the diversity of the training set and improve the generalization of the model. We also normalize the image pixel values to the range of [-1, 1]."
      ],
      "metadata": {
        "id": "squvpbCNXdJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "akemRMZYYyyY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.RandomHorizontalFlip(),\n",
        "     transforms.RandomCrop(32, padding=4),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=100,\n",
        "                                         shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
        "                                        shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mhKKItdXfXF",
        "outputId": "ae9452e1-c572-4c83-aba9-5d74d57a0b8a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will define the CNN model architecture using PyTorch's nn module. The model architecture is a stack of convolutional layers, followed by max pooling layers and fully connected layers. We use the ReLU activation function and the cross-entropy loss function. We also use the SGD optimizer with a learning rate of 0.001 and momentum of 0.9."
      ],
      "metadata": {
        "id": "PsYs-gJrXhgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "net = Net()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n"
      ],
      "metadata": {
        "id": "UGr-K5rTXg63"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we will train the model for 50 epochs and evaluate the model on the test set. We can see that the model achieves an accuracy of about 75% on the test set, which is not bad for a simple CNN model."
      ],
      "metadata": {
        "id": "mW9atRGYYL1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(50):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "    print('Epoch %d loss: %.3f' %\n",
        "          (epoch + 1, running_loss / (i + 1)))\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uuh5y8qNYKNK",
        "outputId": "32562ecd-8d1c-451d-8217-c39af534d713"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 loss: 2.303\n",
            "Epoch 2 loss: 2.297\n",
            "Epoch 3 loss: 2.260\n",
            "Epoch 4 loss: 2.100\n",
            "Epoch 5 loss: 1.982\n",
            "Epoch 6 loss: 1.886\n",
            "Epoch 7 loss: 1.799\n",
            "Epoch 8 loss: 1.738\n",
            "Epoch 9 loss: 1.692\n",
            "Epoch 10 loss: 1.654\n",
            "Epoch 11 loss: 1.616\n",
            "Epoch 12 loss: 1.580\n",
            "Epoch 13 loss: 1.552\n",
            "Epoch 14 loss: 1.529\n",
            "Epoch 15 loss: 1.506\n",
            "Epoch 16 loss: 1.487\n",
            "Epoch 17 loss: 1.475\n",
            "Epoch 18 loss: 1.446\n",
            "Epoch 19 loss: 1.431\n",
            "Epoch 20 loss: 1.411\n",
            "Epoch 21 loss: 1.394\n",
            "Epoch 22 loss: 1.376\n",
            "Epoch 23 loss: 1.363\n",
            "Epoch 24 loss: 1.347\n",
            "Epoch 25 loss: 1.334\n",
            "Epoch 26 loss: 1.320\n",
            "Epoch 27 loss: 1.306\n",
            "Epoch 28 loss: 1.296\n",
            "Epoch 29 loss: 1.278\n",
            "Epoch 30 loss: 1.273\n",
            "Epoch 31 loss: 1.262\n",
            "Epoch 32 loss: 1.251\n",
            "Epoch 33 loss: 1.238\n",
            "Epoch 34 loss: 1.230\n",
            "Epoch 35 loss: 1.218\n",
            "Epoch 36 loss: 1.218\n",
            "Epoch 37 loss: 1.200\n",
            "Epoch 38 loss: 1.199\n",
            "Epoch 39 loss: 1.186\n",
            "Epoch 40 loss: 1.176\n",
            "Epoch 41 loss: 1.172\n",
            "Epoch 42 loss: 1.166\n",
            "Epoch 43 loss: 1.165\n",
            "Epoch 44 loss: 1.155\n",
            "Epoch 45 loss: 1.146\n",
            "Epoch 46 loss: 1.141\n",
            "Epoch 47 loss: 1.135\n",
            "Epoch 48 loss: 1.131\n",
            "Epoch 49 loss: 1.120\n",
            "Epoch 50 loss: 1.119\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluate the model on the test set"
      ],
      "metadata": {
        "id": "bht4sn-SYfwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for data in testloader:\n",
        "    images, labels = data\n",
        "    outputs = net(images)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the test images: %d %%' % (\n",
        "100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EATbiSQYgPe",
        "outputId": "f1aeb9fe-f849-400f-867f-65a62dbe49cf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test images: 59 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In conclusion, we have successfully loaded and preprocessed the CIFAR-10 dataset using PyTorch, and trained and evaluated a simple CNN model for image classification. However, this is just the beginning and there are many ways to improve the performance of the model such as using ResNet, DenseNet and using Data augmentation techniques like Cutout. This is a good starting point for anyone looking to work with image classification using PyTorch."
      ],
      "metadata": {
        "id": "gAUxPH2pYnX_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ySftgRoYocd"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}