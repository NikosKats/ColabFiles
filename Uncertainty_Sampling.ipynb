{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2emZdoejJC3QHXS40LfoH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NikosKats/ColabFiles/blob/Uncertainty-Sampling.ipynb/Uncertainty_Sampling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eC5q2ZFJXrYH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "# Load the CIFAR-10 dataset\n",
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.RandomCrop(32, padding=4),\n",
        "    torchvision.transforms.RandomHorizontalFlip(),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "cifar_dataset = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Define the neural network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "net = Net()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Define the uncertainty sampling strategy\n",
        "def uncertainty_sampling(model, dataset, num_samples):\n",
        "    \"\"\"\n",
        "    Selects the most uncertain samples from the dataset according to the model's predictions.\n",
        "    \"\"\"\n",
        "    uncertain_samples = []\n",
        "    with torch.no_grad():\n",
        "        for data in dataset:\n",
        "            inputs, labels = data\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            prob = torch.softmax(outputs, dim=1)\n",
        "            entropy = -torch.sum(prob * torch.log(prob), dim=1)\n",
        "            uncertain_samples.append((inputs, labels, entropy))\n",
        "    uncertain_samples.sort(key=lambda x: x[2], reverse=True)\n",
        "    return uncertain_samples[:num_samples]\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "num_samples_to_label = 500\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Obtain the most uncertain samples from the training set\n",
        "        uncertain_samples = uncertainty_sampling(net, trainloader, num_samples_to_label)\n",
        "\n",
        "        # Manually label the uncertain samples\n",
        "        inputs, labels = [], []\n",
        "        for sample in uncertain_samples:\n",
        "            inputs.append(sample[0])\n",
        "            labels.append(sample[1])\n",
        "\n",
        "        inputs, labels = torch.stack(inputs), torch.stack(labels)\n",
        "\n",
        "        # Train the model on the labeled samples\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Print the average training loss for the epoch\n",
        "    print(f'Epoch {epoch + 1} loss: {running_loss / (i + 1)}')\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Test the model on the test set\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        inputs, labels = data\n",
        "        outputs = net(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the test images: {100 * correct / total}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code uses the uncertainty sampling strategy to select the most uncertain samples from the training set, and manually labels those samples to improve the model's performance. It then trains the model on the labeled samples, and tests the model's accuracy on the test set.\n",
        "\n",
        "Please note that this is a simple example and the results might not be optimal. You might want to adjust the parameters and try different techniques, for instance using combination of uncertainty, margin, and entropy, or using a deep Q-network strategy."
      ],
      "metadata": {
        "id": "BQd_xO-SX2H1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zYDAqC7gX2jK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}