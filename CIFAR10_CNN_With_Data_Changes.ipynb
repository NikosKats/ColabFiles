{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwWJ6lo6jnOU/mfOHNjtqd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NikosKats/ColabFiles/blob/CIFAR10-CNN-With-Data-Changes.ipynb/CIFAR10_CNN_With_Data_Changes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our second experiment trains the model in the original CIFAR-10 dataset but this time with changes to the dataset by applying horizontal flipping and random cropping to the data with applied padding. Below the code is implemented and explained step by step through the process.\n"
      ],
      "metadata": {
        "id": "ziwH8int-HUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, you will need to import the necessary libraries and set some parameters for the training process:\n"
      ],
      "metadata": {
        "id": "n1B1NHGS5mKm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "frVIdKyU5g1I"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "batch_size = 128\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "use the torchvision.transforms library to apply random horizontal flipping and random cropping to the training images:"
      ],
      "metadata": {
        "id": "QRuMRqt85_1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transforms for the training dataset\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "63A2970E593m"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, you will need to load the CIFAR-10 dataset and apply any necessary preprocessing:"
      ],
      "metadata": {
        "id": "aDU_hOzI5pQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CIFAR-10 dataset with the defined transforms\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='path/to/data', train=True,\n",
        "                                        download=True, transform=train_transform)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='path/to/data', train=False,\n",
        "                                       download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
        "                                          shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
        "                                         shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMDRAJta5nAW",
        "outputId": "94bbffe7-1be0-486f-ea7a-f986b9ff7638"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then you will need to define your model, in this case a convolutional neural network:"
      ],
      "metadata": {
        "id": "8aenwquW5r5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Define the model\n",
        "class CIFAR10Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CIFAR10Model, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(128 * 8 * 8, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 128 * 8 * 8)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the model\n",
        "model = CIFAR10Model()"
      ],
      "metadata": {
        "id": "Ujl06QTN5rQW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then you will need to define a loss function and an optimizer:"
      ],
      "metadata": {
        "id": "wF8PM7o55xkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "WQVbM6vm5vV9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can train your model:"
      ],
      "metadata": {
        "id": "7xudxwzV58Bb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    # Print the current loss\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNaa9ugS56ju",
        "outputId": "b6d52e69-347c-4a1f-956a-852612fad228"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.3393\n",
            "Epoch [2/10], Loss: 1.1556\n",
            "Epoch [3/10], Loss: 1.1895\n",
            "Epoch [4/10], Loss: 1.1708\n",
            "Epoch [5/10], Loss: 1.0087\n",
            "Epoch [6/10], Loss: 1.0044\n",
            "Epoch [7/10], Loss: 1.0286\n",
            "Epoch [8/10], Loss: 1.0977\n",
            "Epoch [9/10], Loss: 0.9385\n",
            "Epoch [10/10], Loss: 1.0533\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the model is trained, you can evaluate its performance on the test dataset:"
      ],
      "metadata": {
        "id": "p7xSYtdr6UVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        \n",
        "    print(f'Accuracy of the model on the test images: {100 * correct / total}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BH5kE3526Uwn",
        "outputId": "3061d828-1679-486a-a224-ee4cde0c925a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model on the test images: 68.93%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we apply the same training in the same dataset but the second time directly to the distorted data we see that the accuracy of the model has 2.37% less accurate results."
      ],
      "metadata": {
        "id": "xKH8_CLvBv54"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d6rvFNis6Yqa"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}