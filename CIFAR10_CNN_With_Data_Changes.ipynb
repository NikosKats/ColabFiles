{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpr7jD0jfbN7JFRzazIlxB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NikosKats/ColabFiles/blob/CIFAR10-CNN-With-Data-Changes.ipynb/CIFAR10_CNN_With_Data_Changes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our second experiment trains the model in the original CIFAR-10 dataset but this time with changes to the dataset by applying horizontal flipping and random cropping to the data with applied padding. Below the code is implemented and explained step by step through the process.\n"
      ],
      "metadata": {
        "id": "ziwH8int-HUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, you will need to import the necessary libraries and set some parameters for the training process:\n"
      ],
      "metadata": {
        "id": "n1B1NHGS5mKm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "frVIdKyU5g1I"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "batch_size = 128\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "use the torchvision.transforms library to apply random horizontal flipping and random cropping to the training images:"
      ],
      "metadata": {
        "id": "QRuMRqt85_1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transforms for the training dataset\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "63A2970E593m"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, you will need to load the CIFAR-10 dataset and apply any necessary preprocessing:"
      ],
      "metadata": {
        "id": "aDU_hOzI5pQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CIFAR-10 dataset with the defined transforms\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='path/to/data', train=True,\n",
        "                                        download=True, transform=train_transform)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='path/to/data', train=False,\n",
        "                                       download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
        "                                          shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
        "                                         shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMDRAJta5nAW",
        "outputId": "94bbffe7-1be0-486f-ea7a-f986b9ff7638"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then you will need to define your model, in this case a convolutional neural network:"
      ],
      "metadata": {
        "id": "8aenwquW5r5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Define the model\n",
        "class CIFAR10Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CIFAR10Model, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(128 * 8 * 8, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 128 * 8 * 8)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the model\n",
        "model = CIFAR10Model()"
      ],
      "metadata": {
        "id": "Ujl06QTN5rQW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then you will need to define a loss function and an optimizer:"
      ],
      "metadata": {
        "id": "wF8PM7o55xkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "WQVbM6vm5vV9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can train your model:"
      ],
      "metadata": {
        "id": "7xudxwzV58Bb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    # Print the current loss\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNaa9ugS56ju",
        "outputId": "b6d52e69-347c-4a1f-956a-852612fad228"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.3393\n",
            "Epoch [2/10], Loss: 1.1556\n",
            "Epoch [3/10], Loss: 1.1895\n",
            "Epoch [4/10], Loss: 1.1708\n",
            "Epoch [5/10], Loss: 1.0087\n",
            "Epoch [6/10], Loss: 1.0044\n",
            "Epoch [7/10], Loss: 1.0286\n",
            "Epoch [8/10], Loss: 1.0977\n",
            "Epoch [9/10], Loss: 0.9385\n",
            "Epoch [10/10], Loss: 1.0533\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the model is trained, you can evaluate its performance on the test dataset:"
      ],
      "metadata": {
        "id": "p7xSYtdr6UVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        \n",
        "    print(f'Accuracy of the model on the test images: {100 * correct / total}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BH5kE3526Uwn",
        "outputId": "3061d828-1679-486a-a224-ee4cde0c925a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model on the test images: 68.93%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we apply the same training in the same dataset but the second time directly to the distorted data we see that the accuracy of the model has 2.37% less accurate results."
      ],
      "metadata": {
        "id": "xKH8_CLvBv54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Achieving an accuracy of 99% on the CIFAR-10 dataset is a challenging task and may require a combination of several techniques. Here are a few suggestions that may help to improve the performance of your model:\n",
        "\n",
        "1. Use a deeper and more complex architecture: A deeper and more complex architecture, such as a ResNet or a DenseNet, can provide more capacity to the model to learn features from the dataset.\n",
        "\n",
        "2. Use pre-trained models: You can use pre-trained models on large datasets like ImageNet, and fine-tune them on your dataset. This can help to leverage the features learned by the pre-trained model and improve the performance on the task.\n",
        "\n",
        "3. Use more advanced data augmentation techniques: You can try more advanced data augmentation techniques such as Cutout, Mixup, and Dropout. These techniques can help to increase the diversity of the training dataset and improve the generalization performance of the model.\n",
        "\n",
        "4. Use a more sophisticated optimizer: You can try more advanced optimization algorithms such as RMSprop, Adagrad, or AdamW which can help to improve the convergence of the model.\n",
        "\n",
        "5. Use ensemble methods: Ensemble methods involve training multiple models and combining their predictions. These methods can help to reduce the variance of the model and improve its performance.\n",
        "\n",
        "6. Hyperparameter tuning: Hyperparameter tuning is the process of fine-tuning the hyperparameters of a model to find the best configuration. You can use techniques such as Grid Search, Random Search, or Bayesian optimization to find the optimal hyperparameters.\n",
        "\n",
        "7. Regularization: Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function. Regularization techniques such as dropout, weight decay, and early stopping can help to reduce overfitting and improve the generalization performance of the model.\n",
        "\n",
        "Keep in mind that achieving an accuracy of 99% on the CIFAR-10 dataset is a challenging task and may require a lot of experimentation and fine-tuning. It's also important to note that, sometimes, even with all these techniques, the accuracy may not reach 99%. But with the combination of these techniques, the accuracy will be close to 99%\n",
        "\n",
        "Finally, I want to remind you that training a model with a high accuracy is not the only goal, you may want to consider other factors such as computational cost, interpretability, and generalization capabilities of the model."
      ],
      "metadata": {
        "id": "WS78gouCxfBl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cutout: Cutout is a regularization technique that randomly masks out a portion of the input images. The following code applies Cutout to the training images with a patch size of 16x16 pixels:"
      ],
      "metadata": {
        "id": "xrohK51syS6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from albumentations import Cutout\n",
        "\n",
        "# Define the Cutout transform\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    Cutout(num_holes=1, max_h_size=16, max_w_size=16, always_apply=False, p=0.5),\n",
        "    transforms.ToTensor()\n",
        "])\n"
      ],
      "metadata": {
        "id": "d6rvFNis6Yqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mixup: Mixup is a regularization technique that generates new training examples by interpolating between two randomly chosen examples. The following code applies Mixup to the training images:"
      ],
      "metadata": {
        "id": "-2PB-qEjyVUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from albumentations.augmentations import functional as F\n",
        "from albumentations import Mixup\n",
        "\n",
        "def mixup_transform(alpha, transform=None):\n",
        "    def augment(data):\n",
        "        data = transform(**data) if transform else data\n",
        "        img, label = data\n",
        "        img1, img2, label1, label2 = F.mixup(img, label, alpha=alpha)\n",
        "        return img1, label1, img2, label2\n",
        "    return augment\n",
        "\n",
        "# Define the Mixup transform\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    Mixup(alpha=1.0, transform=None),\n",
        "    transforms.ToTensor()\n",
        "])\n"
      ],
      "metadata": {
        "id": "9lmuNNJvyU0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropout: Dropout is a regularization technique that randomly sets a portion of the input units to zero during training. The following code applies Dropout to the training images with a drop probability of 0.2:\n",
        "\n"
      ],
      "metadata": {
        "id": "BtknWvnbyZst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from albumentations import Dropout\n",
        "\n",
        "# Define the Dropout transform\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    Dropout(p=0.2, always_apply=False),\n",
        "    transforms.ToTensor()\n",
        "])\n"
      ],
      "metadata": {
        "id": "7GnvEXTxyZEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should also keep in mind that, when using these advanced data augmentation techniques, you may need to adjust the hyperparameters and the architecture of the model accordingly to achieve the best performance. You will need to experiment and fine-tune the model to achieve the best performance."
      ],
      "metadata": {
        "id": "0EfkOdxSyhvm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZNpkUKdsyhds"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}